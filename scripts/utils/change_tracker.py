"""
Change tracking system for monitoring pipeline operations and generating commit messages.

This module provides centralized tracking of changes made during the pipeline execution,
including duplicate removal, hash generation, file processing, and minification operations.
"""

import json
import logging
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Set, Any
from datetime import datetime

logger = logging.getLogger(__name__)


@dataclass
class ChangeStatistics:
    """Statistics for a specific type of change."""
    count: int = 0
    files_affected: Set[str] = field(default_factory=set)
    details: List[str] = field(default_factory=list)


@dataclass
class DomainChanges:
    """Changes made to a specific domain."""
    domain: str
    duplicates_removed: ChangeStatistics = field(default_factory=ChangeStatistics)
    hashes_created: ChangeStatistics = field(default_factory=ChangeStatistics)
    files_sorted: ChangeStatistics = field(default_factory=ChangeStatistics)
    files_minified: ChangeStatistics = field(default_factory=ChangeStatistics)
    image_links: List[str] = field(default_factory=list)


class ChangeTracker:
    """Tracks changes across the entire pipeline execution."""
    
    def __init__(self):
        """Initialize the change tracker."""
        self.domains: Dict[str, DomainChanges] = {}
        self.start_time = datetime.now()
        self.validation_errors_fixed = 0
        self.total_files_processed = 0
        
    def add_domain(self, domain: str) -> DomainChanges:
        """
        Add or get a domain for tracking changes.
        
        Args:
            domain: Domain name (e.g., 'www.xiguashuwu.com')
            
        Returns:
            DomainChanges object for the domain
        """
        if domain not in self.domains:
            self.domains[domain] = DomainChanges(domain=domain)
        return self.domains[domain]
    
    def track_duplicates_removed(self, domain: str, count: int, filename: str, removed_items: List[str]):
        """
        Track duplicate entries removed from filename mappings.
        
        Args:
            domain: Domain name
            count: Number of duplicates removed
            filename: File where duplicates were removed
            removed_items: List of removed items for details
        """
        domain_changes = self.add_domain(domain)
        domain_changes.duplicates_removed.count += count
        domain_changes.duplicates_removed.files_affected.add(filename)
        domain_changes.duplicates_removed.details.extend([f"ç§»é™¤é‡å¤é¡¹: {item}" for item in removed_items])
        
        logger.info(f"è·Ÿè¸ªè®°å½•: {domain} ç§»é™¤äº† {count} ä¸ªé‡å¤é¡¹")
    
    def track_hashes_created(self, domain: str, hash_mapping: Dict[str, str], filename: str):
        """
        Track new hashes created for characters.
        
        Args:
            domain: Domain name
            hash_mapping: Dictionary of new hashes created (hash -> character)
            filename: Hash mapping file that was updated
        """
        domain_changes = self.add_domain(domain)
        count = len(hash_mapping)
        domain_changes.hashes_created.count += count
        domain_changes.hashes_created.files_affected.add(filename)
        
        # Generate image links for each new hash
        for hash_value, character in hash_mapping.items():
            # Create a reference to the image file
            image_link = f"å­—ç¬¦ '{character}' çš„å›¾åƒå“ˆå¸Œ: {hash_value[:16]}..."
            domain_changes.image_links.append(image_link)
            domain_changes.hashes_created.details.append(f"æ–°å»ºå“ˆå¸Œ: {character} -> {hash_value[:16]}...")
        
        logger.info(f"è·Ÿè¸ªè®°å½•: {domain} åˆ›å»ºäº† {count} ä¸ªæ–°å“ˆå¸Œ")
    
    def track_files_sorted(self, domain: str, filename: str, entries_count: int):
        """
        Track file sorting operations.
        
        Args:
            domain: Domain name
            filename: File that was sorted
            entries_count: Number of entries sorted
        """
        domain_changes = self.add_domain(domain)
        domain_changes.files_sorted.count += entries_count
        domain_changes.files_sorted.files_affected.add(filename)
        domain_changes.files_sorted.details.append(f"æ’åºæ–‡ä»¶: {filename} ({entries_count} æ¡ç›®)")
        
        logger.debug(f"è·Ÿè¸ªè®°å½•: {domain} æ’åºäº† {filename}")
    
    def track_files_minified(self, domain: str, source_file: str, target_file: str, size_reduction: int):
        """
        Track file minification operations.
        
        Args:
            domain: Domain name
            source_file: Source file path
            target_file: Minified file path
            size_reduction: Size reduction in bytes
        """
        domain_changes = self.add_domain(domain)
        domain_changes.files_minified.count += 1
        domain_changes.files_minified.files_affected.add(target_file)
        domain_changes.files_minified.details.append(f"å‹ç¼©æ–‡ä»¶: {target_file} (å‡å°‘ {size_reduction} å­—èŠ‚)")
        
        logger.debug(f"è·Ÿè¸ªè®°å½•: {domain} å‹ç¼©äº† {target_file}")
    
    def get_total_changes(self) -> Dict[str, int]:
        """
        Get total changes across all domains.
        
        Returns:
            Dictionary with total change counts
        """
        totals = {
            'duplicates_removed': 0,
            'hashes_created': 0,
            'files_sorted': 0,
            'files_minified': 0,
            'domains_processed': len(self.domains)
        }
        
        for domain_changes in self.domains.values():
            totals['duplicates_removed'] += domain_changes.duplicates_removed.count
            totals['hashes_created'] += domain_changes.hashes_created.count
            totals['files_sorted'] += domain_changes.files_sorted.count
            totals['files_minified'] += domain_changes.files_minified.count
        
        return totals
    
    def has_significant_changes(self) -> bool:
        """
        Check if there are significant changes worth committing.
        
        Returns:
            True if there are significant changes
        """
        totals = self.get_total_changes()
        return (totals['duplicates_removed'] > 0 or 
                totals['hashes_created'] > 0 or 
                totals['files_sorted'] > 0 or 
                totals['files_minified'] > 0)
    
    def generate_commit_title(self) -> str:
        """
        Generate a short, descriptive commit title.
        
        Returns:
            Commit title string
        """
        totals = self.get_total_changes()
        
        if not self.has_significant_changes():
            return "ğŸ”§ ç»´æŠ¤: è¿è¡Œæµæ°´çº¿æ£€æŸ¥"
        
        parts = []
        if totals['hashes_created'] > 0:
            parts.append(f"æ–°å¢{totals['hashes_created']}ä¸ªå“ˆå¸Œ")
        if totals['duplicates_removed'] > 0:
            parts.append(f"æ¸…ç†{totals['duplicates_removed']}ä¸ªé‡å¤é¡¹")
        if totals['files_minified'] > 0:
            parts.append(f"å‹ç¼©{totals['files_minified']}ä¸ªæ–‡ä»¶")
        
        if len(parts) == 0:
            return "ğŸ”„ æ›´æ–°: å¤„ç†æ˜ å°„æ–‡ä»¶"
        
        title = "ğŸ¤– " + "ã€".join(parts)
        if len(title) > 50:  # Keep title short
            title = f"ğŸ¤– å¤„ç†{totals['domains_processed']}ä¸ªåŸŸçš„æ˜ å°„æ–‡ä»¶"
        
        return title
    
    def generate_commit_description(self) -> str:
        """
        Generate a detailed commit description.
        
        Returns:
            Commit description string
        """
        if not self.has_significant_changes():
            return "è¿è¡Œè‡ªåŠ¨åŒ–æµæ°´çº¿æ£€æŸ¥ï¼Œæœªå‘ç°éœ€è¦æ›´æ”¹çš„å†…å®¹ã€‚"
        
        lines = ["è‡ªåŠ¨åŒ–æµæ°´çº¿å¤„ç†ç»“æœ:", ""]
        
        totals = self.get_total_changes()
        
        # Summary section
        lines.append("## å¤„ç†æ‘˜è¦")
        lines.append(f"- å¤„ç†åŸŸæ•°é‡: {totals['domains_processed']}")
        if totals['duplicates_removed'] > 0:
            lines.append(f"- ç§»é™¤é‡å¤é¡¹: {totals['duplicates_removed']} ä¸ª")
        if totals['hashes_created'] > 0:
            lines.append(f"- åˆ›å»ºæ–°å“ˆå¸Œ: {totals['hashes_created']} ä¸ª")
        if totals['files_sorted'] > 0:
            lines.append(f"- æ’åºæ–‡ä»¶: {totals['files_sorted']} ä¸ªæ¡ç›®")
        if totals['files_minified'] > 0:
            lines.append(f"- å‹ç¼©æ–‡ä»¶: {totals['files_minified']} ä¸ª")
        
        lines.append("")
        
        # Domain-specific details
        for domain, changes in self.domains.items():
            if self._has_domain_changes(changes):
                lines.append(f"## {domain}")
                
                if changes.duplicates_removed.count > 0:
                    lines.append(f"### ç§»é™¤é‡å¤é¡¹ ({changes.duplicates_removed.count})")
                    for detail in changes.duplicates_removed.details[:5]:  # Limit details
                        lines.append(f"- {detail}")
                    if len(changes.duplicates_removed.details) > 5:
                        lines.append(f"- ... è¿˜æœ‰ {len(changes.duplicates_removed.details) - 5} é¡¹")
                    lines.append("")
                
                if changes.hashes_created.count > 0:
                    lines.append(f"### æ–°å»ºå“ˆå¸Œ ({changes.hashes_created.count})")
                    for detail in changes.hashes_created.details[:5]:  # Limit details
                        lines.append(f"- {detail}")
                    if len(changes.hashes_created.details) > 5:
                        lines.append(f"- ... è¿˜æœ‰ {len(changes.hashes_created.details) - 5} é¡¹")
                    lines.append("")
                
                if changes.image_links:
                    lines.append("### å›¾åƒæ–‡ä»¶å¼•ç”¨")
                    for link in changes.image_links[:3]:  # Limit links
                        lines.append(f"- {link}")
                    if len(changes.image_links) > 3:
                        lines.append(f"- ... è¿˜æœ‰ {len(changes.image_links) - 3} ä¸ªå›¾åƒå“ˆå¸Œ")
                    lines.append("")
        
        # Processing time
        duration = datetime.now() - self.start_time
        lines.append(f"å¤„ç†æ—¶é—´: {duration.total_seconds():.1f} ç§’")
        
        return "\n".join(lines)
    
    def _has_domain_changes(self, changes: DomainChanges) -> bool:
        """Check if a domain has any significant changes."""
        return (changes.duplicates_removed.count > 0 or 
                changes.hashes_created.count > 0 or 
                changes.files_sorted.count > 0 or 
                changes.files_minified.count > 0)
    
    def export_to_json(self, output_path: Path):
        """
        Export change tracking data to JSON file for workflow consumption.
        
        Args:
            output_path: Path to output JSON file
        """
        data: Dict[str, Any] = {
            'timestamp': datetime.now().isoformat(),
            'has_changes': self.has_significant_changes(),
            'commit_title': self.generate_commit_title(),
            'commit_description': self.generate_commit_description(),
            'totals': self.get_total_changes(),
            'domains': {}
        }
        
        for domain, changes in self.domains.items():
            data['domains'][domain] = {
                'duplicates_removed': changes.duplicates_removed.count,
                'hashes_created': changes.hashes_created.count,
                'files_sorted': changes.files_sorted.count,
                'files_minified': changes.files_minified.count,
                'image_links': changes.image_links
            }
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            logger.info(f"å˜æ›´è·Ÿè¸ªæ•°æ®å·²å¯¼å‡ºåˆ°: {output_path}")
        except Exception as e:
            logger.error(f"å¯¼å‡ºå˜æ›´è·Ÿè¸ªæ•°æ®å¤±è´¥: {e}")
